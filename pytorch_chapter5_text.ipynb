{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245a24e6",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks/LSTMs\n",
    "\n",
    "This notebook demonstrates text classification using modern PyTorch and Hugging Face libraries.\n",
    "\n",
    "**Note:** We're using `transformers` and `datasets` instead of the deprecated `torchtext` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb1508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuffPost dataset...\n",
      "Number of labels: 37\n",
      "Loaded: Train=140255, Val=30055, Test=30055\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c97a540d044c1b86dd1e4ee1bcd887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/140255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdb12aa483e4fa08ca7ba33fbf6479f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75233947b3294464b6ea781b6b29b4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "# load in huffpost dataset from project \n",
    "from datasets import load_from_disk\n",
    "import pickle\n",
    "\n",
    "print(\"Loading HuffPost dataset...\")\n",
    "processed_datasets = load_from_disk(\"huffpost_processed_milestone2\")\n",
    "train_ds = processed_datasets['train']\n",
    "val_ds = processed_datasets['validation']\n",
    "test_ds = processed_datasets['test']\n",
    "\n",
    "# Load label encoder\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "# Load class weights\n",
    "class_weights = np.load('class_weights.npy')\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Loaded: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n",
    "\n",
    "# Initialize tokenizer for DistilBERT\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_ds.map(tokenize_function, batched=True, batch_size=1000)\n",
    "val_dataset = val_ds.map(tokenize_function, batched=True, batch_size=1000)\n",
    "test_dataset = test_ds.map(tokenize_function, batched=True, batch_size=1000)\n",
    "\n",
    "# Set format for PyTorch - this converts to tensors\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lstm-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 66,981,925 parameters\n",
      "Trainable parameters: 66,981,925\n",
      "Device: mps\n",
      "Output classes: 37\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained DistilBERT model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "print(\"Loading pretrained DistilBERT model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Output classes: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "training-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n",
      "Train batches per epoch: 8766\n",
      "Val batches per epoch: 1879\n",
      "Test batches per epoch: 1879\n",
      "Learning rate: 2e-05\n",
      "Total training steps: 26298\n",
      "Using weighted loss for 37 classes\n"
     ]
    }
   ],
   "source": [
    "# Training setup for DistilBERT\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "BATCH_SIZE = 16  # Smaller batch size for transformer\n",
    "LEARNING_RATE = 2e-5  # Standard learning rate for BERT fine-tuning\n",
    "EPOCHS = 3  # 3-5 epochs typical for fine-tuning\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Loss and optimizer\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Use AdamW optimizer (standard for transformers)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,  # 10% warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Training setup complete!\")\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Val batches per epoch: {len(val_loader)}\")\n",
    "print(f\"Test batches per epoch: {len(test_loader)}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Using weighted loss for {len(class_weights_tensor)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "training-loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DistilBERT fine-tuning...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8766/8766 [18:00<00:00,  8.11it/s, loss=1.821, acc=54.0%]\n",
      "Evaluating: 100%|██████████| 1879/1879 [01:03<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3:\n",
      "  Train Loss: 1.805 | Train Acc: 54.03%\n",
      "  Val Loss: 1.262 | Val Acc: 64.11%\n",
      "  ✓ New best model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8766/8766 [18:00<00:00,  8.11it/s, loss=1.089, acc=68.2%]\n",
      "Evaluating: 100%|██████████| 1879/1879 [01:01<00:00, 30.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3:\n",
      "  Train Loss: 1.047 | Train Acc: 68.25%\n",
      "  Val Loss: 1.189 | Val Acc: 66.76%\n",
      "  ✓ New best model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8766/8766 [17:48<00:00,  8.20it/s, loss=0.130, acc=74.9%]\n",
      "Evaluating: 100%|██████████| 1879/1879 [01:04<00:00, 29.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3:\n",
      "  Train Loss: 0.772 | Train Acc: 74.87%\n",
      "  Val Loss: 1.210 | Val Acc: 67.98%\n",
      "  ✓ New best model saved!\n",
      "\n",
      "Training complete! Best validation accuracy: 67.98%\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1879/1879 [01:02<00:00, 30.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.238 | Test Acc: 67.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop for DistilBERT\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass - model returns a dict with 'logits'\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.3f}', 'acc': f'{100*correct/total:.1f}%'})\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting DistilBERT fine-tuning...\\n\")\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        model.save_pretrained('best_distilbert_model')\n",
    "        tokenizer.save_pretrained('best_distilbert_model')\n",
    "        print(f\"  ✓ New best model saved!\")\n",
    "    print()\n",
    "\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('best_distilbert_model').to(device)\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20180cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmetal",
   "language": "python",
   "name": "tfmetal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
