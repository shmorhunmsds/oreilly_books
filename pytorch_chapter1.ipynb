{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deac2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523888d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9657f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7030dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5621, 0.1849],\n",
      "        [0.5342, 0.7369]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfe788",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "A tensor is both a container for numbersas well as a set of rules that define transformations between tensors that produce new tensors. \n",
    "\n",
    "Easiest to think about tensors as multidimensional arrays\n",
    "\n",
    "Every tensor has a rank that corresponds to its dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c6a27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0,0,1], [1,1,1], [0,0,0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb33023",
   "metadata": {},
   "source": [
    "we can change an element in a tensor by using standard python indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c71138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 0, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0] = 5\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028bc18",
   "metadata": {},
   "source": [
    "special functions can generate particular types of tensors, such as `ones` and `zeroes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1224a8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62699527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, 2) + torch.ones(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ea3e0",
   "metadata": {},
   "source": [
    "and if you have a tensor of rank 0, you can pull  out the value with `item()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b8ca77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29672127962112427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ca7cb",
   "metadata": {},
   "source": [
    "tensors can live in the cpu and on the gpu and be copied between devices by using the `to()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ccea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "cpu_tensor = torch.rand(2)\n",
    "print(cpu_tensor.device)\n",
    "mps_tensor = cpu_tensor.to('mps')\n",
    "print(mps_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c78d2c",
   "metadata": {},
   "source": [
    "## Tensor Operations\n",
    "\n",
    "1. Find the maximum item in a tensor as well as the index that contains the maximum value, done with the max() and argmax() functions. We can also use item() to extract a standard python value from a 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b594695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9931)\n",
      "0.6906684041023254\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(2,2).max())\n",
    "print(torch.rand(2,2).max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f04a54",
   "metadata": {},
   "source": [
    "Sometimes we want to change the type of a tensor - such as from a long tensor to a float tensor. We can do this with `to():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f2aac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "long_tensor = torch.tensor([[0,0,1], [1,1,1], [0,0,0]])\n",
    "print(long_tensor.type())\n",
    "\n",
    "float_tensor = torch.tensor([[0,0,1], [1,1,1], [0,0,0]]).to(dtype=torch.float32)\n",
    "print(float_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3ead6",
   "metadata": {},
   "source": [
    "If you want to save memory you can use an *in place* funcction:\n",
    "\n",
    "Look to see if an in place function is defined, which should be the same name as the original function but with an appended underscore(_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed2ea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5836, -0.0632],\n",
       "        [-1.5603, -2.7241]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.rand(2,2)\n",
    "random_tensor.log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef67a68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5836, -0.0632],\n",
       "        [-1.5603, -2.7241]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.log2_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b580576",
   "metadata": {},
   "source": [
    "another common operation is reshaping a tensor. This can occur because your NN layer may require a slightly different input shape than that you currently have to feed into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6292b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "flat_tensor = torch.rand(784)\n",
    "print(flat_tensor.shape)\n",
    "\n",
    "viewed_tensor = flat_tensor.view(1, 28, 28)\n",
    "print(viewed_tensor.shape)\n",
    "\n",
    "reshaped_tensor = flat_tensor.reshape(1, 28, 28)\n",
    "print(reshaped_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e0f9f",
   "metadata": {},
   "source": [
    "note that the reshaped tensor has to ahve he same number of total elements as the original, if you try `flat_tensor.reshape(3, 28, 28)`, you'll see an error.\n",
    "\n",
    "#### view vs reshape\n",
    "- view operatesas a view of the original tensor, so if the underlying data is changed, teh view will change too. \n",
    "- view can throw errors if the required block is not *contiguous* (it doesn't share the same block of memory it would occupy if a new tensor of the required shape was created from scratch). \n",
    "- IF this happens, you have to call `tensor.contiguous()` before you can use `view()`. \n",
    "\n",
    "However, reshape does it all behind the scenes.\n",
    "\n",
    "### Rearranging the dimensions of a tensor\n",
    "You will likely come across this with images, which are often stored as `[height, width, channel]` tensors, but pytorch prefers to deal with these in a `[channel, height, width]`. \n",
    "\n",
    "You can use `permute` to deal with these in a fairly straightforward manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa1551a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 640, 480])\n"
     ]
    }
   ],
   "source": [
    "hwc_tensor = torch.rand(640, 480, 3)\n",
    "chw_tensor = hwc_tensor.permute(2, 0, 1)\n",
    "print(chw_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24bad8",
   "metadata": {},
   "source": [
    "## Tensor Broadcasting\n",
    "Broadcasting allows you to perform operations between a tensor and a smaller tensor. You can boradcast acrosstwo tensors if, starting backward from their trailing dimensions:\n",
    "- the two dimensions are equal\n",
    "- One of the dimensions is 1.\n",
    "\n",
    "In our use of broadcasting, it works because 1 has a dimensions of 1, and there are no other dimensions, the 1 can be expanded to cover the other tensor.\n",
    "\n",
    "If we tried to add a `[2,2]` tensor to a `[3,3]` tensor, we'd get an error message, but we could add a `[1,3]` tensor to the `[3,3]` tensor without any trouble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db826115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
